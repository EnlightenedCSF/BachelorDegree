\subsection{Реализация алгоритма локализации \\по Монте-Карло}

Метод также известен как <<локализация с помощью фильтра частиц>>.

В рамках данного метода алгоритм генерирует сотни частиц, представляющих возможные будущие положения пользователя. Вычисляются расстояния до маяков и учитываются значения их сигналов. На основании этих актуальных показаний и несоответствия предсказаний этим данным, часть частиц фильтруется, то есть выходит из рассмотрения, а оставшаяся часть участвует в дальнейших вычислениях. Это происходит в результате обновления весов, присвоенных каждой из частиц.

Проблему локализации можно сформулировать как проблему определения апостериорной вероятности  $p(x_k | z_{1:k})$. Начальное распределение $p(x_0)$ считается равномерным по всем возможным локациям $[x \enskip y]^T$.

Рассматривая локализацию по Монте-Карло, требуемое распределение \\$p(x_k | z_{1:k})$ может быть представлено в виде множества взвешенных значений:
\[
    S_k = \{ x^i, \omega^i \}, i = 0,1,2,...,N_p
\]
где $\omega^i$ - вес частицы - представляет собой фактор важности, для которого выполняется равенство $\sum_{i=1}^{N_p} \omega_i = 1$. 

Апостериорная вероятность может быть аппроксимирована с помощью следующего выражения:
\[
    p(x_k | z_k) \approx \frac{1}{N_p} \sum \delta (x_k - x_k^i)
\]
где $\delta$ - дельта-функция Дирака. Для достаточно большого $N_p$ аппроксимация отклоняется от истинного значения искомого распределения незначительно.

Алгоритм функционирует в несколько стадий:
\begin{enumerate}
    \item
    \textbf{Предсказание}. На этом этапе апостериорное распределение \\$p(x_k | x_{k-1}, u_{k-1})$ в момент времени $k$ предсказывается на основании состояния $p(x_{k-1} | z_{1:k-1})$ и управляющего вектора $u_{k-1}$. Множество частиц $S_{k-1}$ соответствует состоянию $x_{k-1}$. Вектор $u_{k-1}$ должен быть применен к каждой частице из $S_{k-1}$. Это дает новый набор $S'_k = x'^i, \quad \omega'^i, \\ i=0,1,2,..., N_p$. Заметим, что $\omega'^i = \omega^i$.
    \item
    \textbf{Обновление}. На этом шаге принимается во внимание модель измерений: каждая частица из $S'_k$ изменяет вес на основании степени схожести с $p(z_k | x^i_k), \quad i=0,1,2,...,N_p$. После этого образуется новый набор частиц $S_k$.
    \item
    \textbf{Вырождение}. После нескольких итераций большинство частиц имеют незначительный вес, и лишь немногие из них вносят значительный вклад в вычисления. Это стало причиной разработок многих алгоритмов ресэмплинга, позволяющих выделить только необходимую часть частиц. Чтобы избежать накладных расходов на ресэмплинг во время каждой из итераций, зачастую вычисляется Effective Sample Size, ESS. На основании этого функционала, ресэмплинг применяется лишь в случае, когда величина ESS опускается ниже определенного порога. Ресемплинг зачастую, помима отсева незначащих частиц, копирует те, значения весов которых значительны.
    \begin{align}
        cv_t^2 &= \frac{var(\omega_t^i)}{ E^2(\omega_t^i) } = \frac{1}{N_p} \sum_{i=1}^{N_p} (N_p \omega^i - 1)^2 \notag \\
        ESS_t &= \frac{N_p}{(1+cv_t^2)} \notag
    \end{align}
\end{enumerate}

Однако, в рамках решения задачи локализации с помощью данного метода возникает так называемая \textit{проблема потери частиц} (\textit{"particle de\-pri\-va\-tion"}). Суть этой проблемы, подробно описанной в \cite{thrun2001robust}, сводится к тому, что алгоритм во время проведения этапа ресэмплинга может отбросить частицы, являющиеся значимыми для локализации. Это особенно значимо для вариантов реализации с небольшим ($M \leq 50$) числом частиц.

В качестве возможного, хотя и не однозначного решения возможно генерировать дополнительно некоторое количество частиц каждую итерацию работы алгоритма. 

Тем не менее, из-за неустойчивости в формировании результата данным алгоритмом, было решено в рамках реализации библиотеки не включать данный метод.